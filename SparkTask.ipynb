{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqZirpef6Pc6",
        "outputId": "8dde8c00-32ae-4ce3-9aef-88ee5aa17ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=64f3a4bff61271ca923ce6fec53c095b6e20c8478693e4d10b3418c3d14c4f97\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, split\n",
        "import time\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from operator import add\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Wikimedia Statistics\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "# Load data\n",
        "data_path = \"/content/drive/MyDrive/Databases/pagecounts-20160101-000000_parsed.out\"  # Update this to the path of your file\n",
        "df = spark.read.text(data_path)\n",
        "\n",
        "# Define a schema for the DataFrame\n",
        "df = df.withColumn(\"page_size\", split(col(\"value\"), \" \")[3].cast(\"long\"))\n",
        "\n",
        "# Sample or limit data to reduce the size and manage memory usage\n",
        "df = df.limit(3324129)  # Adjust the limit based on the memory capacity of your system\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "# Compute statistics using Spark loops\n",
        "total_page_sizes = 0\n",
        "min_page_size = float('inf')\n",
        "max_page_size = float('-inf')\n",
        "num_page_sizes = 0\n",
        "\n",
        "for row in df.collect():\n",
        "    page_size = row.page_size\n",
        "    total_page_sizes += page_size\n",
        "    num_page_sizes += 1\n",
        "    if page_size < min_page_size:\n",
        "        min_page_size = page_size\n",
        "    if page_size > max_page_size:\n",
        "        max_page_size = page_size\n",
        "\n",
        "# Avoid division by zero if no rows are returned\n",
        "if num_page_sizes > 0:\n",
        "    avg_page_size = total_page_sizes / num_page_sizes\n",
        "else:\n",
        "    avg_page_size = 0\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(\"Q1 loop\\n\")\n",
        "print(\"Elapsed time:\", elapsed_time, \"seconds\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load data\n",
        "data_path = \"/content/drive/MyDrive/Databases/pagecounts-20160101-000000_parsed.out\"\n",
        "df = spark.read.text(data_path)\n",
        "\n",
        "# Define a function to extract project code and page size\n",
        "def extract_project_size(line):\n",
        "    fields = line.split(\" \")\n",
        "    # Make sure there are enough fields and they are numeric\n",
        "    if len(fields) > 3 and fields[3].isdigit():\n",
        "        return int(fields[3])\n",
        "    return 0  # Default value if not enough fields or not a digit\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Map phase: Extract page sizes\n",
        "page_sizes_rdd = df.rdd.map(lambda row: row.value).map(extract_project_size)\n",
        "\n",
        "# Reduce phase: Compute statistics\n",
        "total_page_sizes = page_sizes_rdd.sum()\n",
        "num_page_sizes = page_sizes_rdd.count()\n",
        "min2_page_size = page_sizes_rdd.min()\n",
        "max2_page_size = page_sizes_rdd.max()\n",
        "avg2_page_size = total_page_sizes / num_page_sizes if num_page_sizes > 0 else 0\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "\n",
        "print(\"Q1 map-reduce\\n\")\n",
        "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save results to Google Drive\n",
        "output_path = \"/content/drive/MyDrive/Results/output.txt\"\n",
        "with open(output_path, 'w') as file:\n",
        "    file.write(\"Q1 Loop: \\n\")\n",
        "    file.write(\"Min Page Size: \" + str(min_page_size) + \"\\n\")\n",
        "    file.write(\"Max Page Size: \" + str(max_page_size) + \"\\n\")\n",
        "    file.write(\"Avg Page Size: \" + str(avg_page_size) + \"\\n\")\n",
        "    file.write(\"Q1 Map-Reduce: \\n\")\n",
        "    file.write(\"Min Page Size: \" + str(min2_page_size) + \"\\n\")\n",
        "    file.write(\"Max Page Size: \" + str(max2_page_size) + \"\\n\")\n",
        "    file.write(\"Avg Page Size: \" + str(avg2_page_size) + \"\\n\")\n",
        "    file.write(\"------------------------------------------------------------------------\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlYyx9tCyXv0",
        "outputId": "a1da6723-01df-46dc-edc0-6fe6823b8eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Q1 loop\n",
            "\n",
            "Elapsed time: 57.52911972999573 seconds\n",
            "\n",
            "Q1 map-reduce\n",
            "\n",
            "Elapsed time: 110.9327871799469 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load data\n",
        "data_path = \"/content/drive/MyDrive/Databases/pagecounts-20160101-000000_parsed.out\"\n",
        "df = spark.read.text(data_path)\n",
        "\n",
        "# Define a schema for the DataFrame\n",
        "df = df.withColumn(\"project_code\", split(col(\"value\"), \" \")[0]) \\\n",
        "       .withColumn(\"page_title\", split(col(\"value\"), \" \")[1]) \\\n",
        "       .withColumn(\"page_hits\", split(col(\"value\"), \" \")[2].cast(\"long\")) \\\n",
        "       .withColumn(\"page_size\", split(col(\"value\"), \" \")[3].cast(\"long\")) \\\n",
        "       .drop(\"value\")\n",
        "\n",
        "# Sample or limit data to reduce the size\n",
        "sampled_df = df.limit(3324129)  # Adjust this number based on your environment's capacity\n",
        "\n",
        "# Initialize counters\n",
        "titles_starting_with_the = 0\n",
        "english_project_titles_starting_with_the = 0\n",
        "\n",
        "# Loop through a more manageable subset of data\n",
        "for row in sampled_df.collect():\n",
        "    if row.page_title.startswith(\"The_\"):\n",
        "        titles_starting_with_the += 1\n",
        "        if row.project_code == \"en\":\n",
        "            english_project_titles_starting_with_the += 1\n",
        "\n",
        "# Calculate the number of titles starting with \"The\" that are not part of the English project\n",
        "non_english_project_titles_starting_with_the = titles_starting_with_the - english_project_titles_starting_with_the\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(\"Q2 loop:\")\n",
        "print(\"Elapsed time:\", elapsed_time, \"seconds\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load data\n",
        "data_path = \"/content/drive/MyDrive/Databases/pagecounts-20160101-000000_parsed.out\"\n",
        "df = spark.read.text(data_path)\n",
        "\n",
        "# Define a function to extract project code and page title\n",
        "def extract_project_title(line):\n",
        "    fields = line.split(\" \")\n",
        "    return (fields[0], fields[1])\n",
        "\n",
        "# Map phase: Extract project code and page title\n",
        "project_title_rdd = df.rdd.map(lambda row: row.value).map(extract_project_title)\n",
        "\n",
        "# Reduce phase: Count page titles starting with \"The\"\n",
        "titles_starting_with_the_2 = project_title_rdd.filter(lambda x: x[1].startswith(\"The_\")).count()\n",
        "\n",
        "# Reduce phase: Count page titles starting with \"The\" that are part of the English project\n",
        "english_project_titles_starting_with_the_2 = project_title_rdd.filter(lambda x: x[1].startswith(\"The_\") and x[0] == \"en\").count()\n",
        "\n",
        "# Calculate the number of titles starting with \"The\" that are not part of the English project\n",
        "non_english_project_titles_starting_with_the_2 = titles_starting_with_the_2 - english_project_titles_starting_with_the_2\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print results to console\n",
        "print(\"Q2 map reduce\")\n",
        "print(\"Elapsed time:\", elapsed_time, \"seconds\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Save or append results to Google Drive\n",
        "output_path = \"/content/drive/MyDrive/Results/output.txt\"\n",
        "with open(output_path, 'a') as file:  # 'a' is for append mode; use 'a+' if you also need to read\n",
        "    file.write(\"Q2 Map-Reduce: \\n\")\n",
        "    file.write(\"Number of page titles starting with 'The': \" + str(titles_starting_with_the_2) + \"\\n\")\n",
        "    file.write(\"Number of those page titles that are not part of the English project: \" + str(non_english_project_titles_starting_with_the_2) + \"\\n\")\n",
        "\n",
        "    file.write(\"Q2 Loop: \\n\")\n",
        "    file.write(\"Number of page titles starting with 'The': \" + str(titles_starting_with_the) + \"\\n\")\n",
        "    file.write(\"Number of those page titles that are not part of the English project: \" + str(non_english_project_titles_starting_with_the) + \"\\n\")\n",
        "    file.write(\"------------------------------------------------------------------------\\n\")\n",
        "\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "nmg-y-mPPdIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2abdd547-3943-4b64-a36e-6c731eba4610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q2 loop:\n",
            "Elapsed time: 53.31073784828186 seconds\n",
            "\n",
            "Q2 map reduce\n",
            "Elapsed time: 57.76884698867798 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import time\n",
        "from operator import add\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"Wikimedia Project\")\n",
        "sc = SparkContext(conf=conf)\n",
        "data = sc.textFile(\"/content/drive/MyDrive/Databases/pagecounts-20160101-000000_parsed.out\")#/content/drive/MyDrive/pagecounts-20160101-000000_parsed.out"
      ],
      "metadata": {
        "id": "ivTKFhD1Rjfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5591f00f-6375-4c8f-8779-546ed19cd19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3"
      ],
      "metadata": {
        "id": "aWfx_N-N6U-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "page_title_rdd = data.map(lambda line: line.split(\" \")[1] if len(line.split(\" \")) >= 2 else line.split(\" \")[0])\n",
        "\n",
        "\n",
        "splited_title = page_title_rdd.flatMap(lambda line: line.split('_'))\n",
        "\n",
        "\n",
        "normalized_splited_title = splited_title.map(lambda line: line.lower())\n",
        "normalized_splited_title = normalized_splited_title.map(lambda line: ''.join(ch for ch in line if ch.isalnum()))\n",
        "\n",
        "from operator import add\n",
        "term_count = normalized_splited_title.map(lambda term: (term,1)).reduceByKey(add)\n",
        "distinct_terms = term_count.filter(lambda term: term[1] == 1)\n",
        "count = distinct_terms.count()\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(\"Q3 map reduce:\")\n",
        "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# get page titles\n",
        "start_time = time.time()\n",
        "title_list = []\n",
        "for line in page_title_rdd.toLocalIterator():\n",
        "  if len(line.split(\" \")) >= 2:\n",
        "    title = line.split(\" \")[1]\n",
        "    title_list.append(title)\n",
        "\n",
        "# split titles into words\n",
        "\n",
        "splited_title = []\n",
        "for line in page_title_rdd.toLocalIterator():\n",
        "    words = line.split('_')\n",
        "    splited_title.extend(words)\n",
        "\n",
        "normalized_terms = []\n",
        "for term in splited_title:\n",
        "    term_normalized = term.lower()\n",
        "    term_normalized = ''.join(ch for ch in term_normalized if ch.isalnum())\n",
        "    normalized_terms.append(term_normalized)\n",
        "\n",
        "\n",
        "# normalize terms\n",
        "\n",
        "# count distinct terms\n",
        "term_count = {}\n",
        "for term in normalized_splited_title.toLocalIterator():\n",
        "    if term not in term_count:\n",
        "        term_count[term] = 1\n",
        "    else:\n",
        "        term_count[term] += 1\n",
        "\n",
        "distinct_terms = sc.parallelize([(term, count) for term, count in term_count.items() if count == 1])\n",
        "\n",
        "# get count of distinct terms\n",
        "count2 = 0\n",
        "for term in distinct_terms.toLocalIterator():\n",
        "    count2 += 1\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(\"Q3 Loop:\")\n",
        "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
        "\n",
        "\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/Results/output.txt\"\n",
        "with open(output_path, 'a') as file:  # 'a' is for append mode; use 'a+' if you also need to read\n",
        "    file.write(\"Q3 Map-Reduce: \\n\")\n",
        "    file.write(\" count: \"+ str(count) + \"\\n\")\n",
        "    file.write(\"Q3 Loop: \\n\")\n",
        "    file.write(\"count: \" + str(count2) + \"\\n\")\n",
        "    file.write(\"------------------------------------------------------------------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KCeY9p35aAA",
        "outputId": "13261271-2121-4364-813a-f1b788cb4145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q3 map reduce:\n",
            "Elapsed time: 55.09373617172241 seconds\n",
            "Q3 Loop:\n",
            "Elapsed time: 85.36036324501038 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4"
      ],
      "metadata": {
        "id": "gBXmqciI6V3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_title_rdd = data.map(lambda line: line.split(\" \")[1] if len(line.split(\" \")) >= 2 else line.split(\" \")[0])\n",
        "\n",
        "start_time = time.time()\n",
        "# Count the frequency of each title\n",
        "freq_title = page_title_rdd.map(lambda term: (term,1)).reduceByKey(lambda a,b:a +b)\n",
        "most_freq_title = freq_title.sortBy(lambda pair: pair[1], False)\n",
        "\n",
        "# Print the results using foreach\n",
        "#most_freq_title.foreach(lambda x: print(f\"({x[0]}, {x[1]})\"))\n",
        "\n",
        "#most_freq_title.filter(lambda x: x[0]).saveAsTextFile('./q4544451')\n",
        "# most_freq = most_freq_title.first()\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(\"Question 4\")\n",
        "print(\"Map-Reduce\")\n",
        "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Count the frequency of each title\n",
        "title_counts = {}\n",
        "for line in data.toLocalIterator():\n",
        "    fields = line.split(\" \")\n",
        "    title = fields[1] if len(fields) >= 2 else fields[0]\n",
        "    if title not in title_counts:\n",
        "        title_counts[title] = 1\n",
        "    else:\n",
        "        title_counts[title] += 1\n",
        "\n",
        "# Sort the title counts in descending order\n",
        "sorted_counts = sorted(title_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(\"Spark Loops\")\n",
        "print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
        "\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/Results/output.txt\"\n",
        "with open(output_path, 'a') as file:  # 'a' is for append mode; use 'a+' if you also need to read\n",
        "    file.write(\"Q4 Map-Reduce: \\n\")\n",
        "    file.write(\" counts: \\n\" + str(most_freq_title.take(20)) + \" seconds\\n\")\n",
        "    file.write(\"Q4 Loop: \\n\")\n",
        "    file.write(\"counts: \\n\" + str(sorted_counts[:20]) + \" seconds\\n\")\n",
        "    file.write(\"------------------------------------------------------------------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19bUKV7i6WJC",
        "outputId": "3a336b28-ca91-413f-a1f9-b2ecbf559312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 4\n",
            "Map-Reduce\n",
            "Elapsed time: 36.57109260559082 seconds\n",
            "Spark Loops\n",
            "Elapsed time: 38.06556558609009 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5"
      ],
      "metadata": {
        "id": "NbAl0egf6WnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fields = data.map(lambda line: line.split(\" \"))\n",
        "\n",
        "#using for loops\n",
        "startTime1 = time.time()\n",
        "pages ={}\n",
        "for f in fields.collect():\n",
        "\n",
        "  if f[1] in pages:\n",
        "    pages[f[1]] += \", page hits: \"+str(f[2])+\" page size: \"+str(f[3])\n",
        "\n",
        "  else:\n",
        "    pages[f[1]] = \"page hits: \"+str(f[2])+\" page size: \"+str(f[3])\n",
        "\n",
        "endTime1 = time.time()\n",
        "\n",
        "#using map reduce\n",
        "startTime2 = time.time()\n",
        "\n",
        "titles = fields.map(lambda field: (field[1],field[2]+\" \"+field[3]))\n",
        "resultRdd = titles.reduceByKey(lambda d1,d2 : d1+\" , \"+d2)\n",
        "result = resultRdd.collect()\n",
        "\n",
        "endTime2 = time.time()\n",
        "\n",
        "print(\"Question 5\")\n",
        "print(\"Spark loops\")\n",
        "print(endTime1 - startTime1,\"seconds\")\n",
        "print(\"Map-Reduce\")\n",
        "print(endTime2 - startTime2,\"seconds\")\n",
        "\n",
        "print(result[:20])\n",
        "\n",
        "\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/Results/output.txt\"\n",
        "with open(output_path, 'a') as file:  # 'a' is for append mode; use 'a+' if you also need to read\n",
        "    file.write(\"Q5 Map-Reduce: \\n\")\n",
        "    file.write(\"pages with same titles: \\n\")\n",
        "    for r in result[:50]:\n",
        "      file.write(str(r) +\"\\n\")\n",
        "    file.write(\"Q5 Loop: \\n\")\n",
        "    first_50_pages = list(pages.items())[:50]\n",
        "    for key, value in first_50_pages:\n",
        "        file.write(f\"{key}: {value}\\n\")\n",
        "    file.write(\"------------------------------------------------------------------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqmLTxL36W48",
        "outputId": "335173e1-b239-40db-c121-d900aca94dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 5\n",
            "Spark loops\n",
            "25.43810534477234 seconds\n",
            "Map-Reduce\n",
            "32.34749627113342 seconds\n",
            "[('E.Desv', '1 4662 , 1 5210 , 1 4825 , 1 5237 , 1 7057 , 1 4548'), ('Special:Contributions/5.232.61.79', '1 5805'), ('Special:ListFiles/Nyttend', '1 5032'), ('Special:WhatLinksHere/Main_Page', '1 5556 , 2 15231 , 5 101406 , 1 8597 , 1 8550 , 1 11529 , 1 5698 , 3 32145'), ('Time_Inc', '1 4672 , 1 6182 , 1 4842 , 1 4923'), ('Special:Contributions/MBisanz', '1 5674'), ('Special:UserLogin', '1 4899 , 30 181938 , 44198 718770014 , 4 34449 , 1 5221 , 13 58547 , 3 12523 , 2 8696 , 1 5311 , 2 9960 , 1 5052 , 5 49952 , 1 4989'), ('Acanthophorus_serraticornis', '1 5942 , 1 5825 , 1 5480 , 1 5510 , 1 5429 , 1 4693'), ('Allen_R._Schindler,_Jr', '1 5957 , 1 5840 , 1 4711 , 1 4900 , 1 4881 , 1 7322'), ('Annales._Histoire,_Sciences_Sociales/en/Annales_d', '1 6008 , 1 7775 , 1 5678 , 1 6100'), ('N.P.R', '1 5900 , 1 5438 , 1 5000 , 1 4853 , 1 4659 , 1 6338 , 1 7159'), ('Nord-Pas-de-Calais', '1 5925 , 1 5806 , 1 5460 , 4 120526 , 2 168302 , 1 14875 , 8 450792 , 1 17938 , 1 18515 , 1 7716'), ('Agnes_Monica', '1 13278 , 1 44900 , 1 16203 , 1 12184 , 1 72848'), ('Apridar', '1 6447'), ('Asia', '1 25859 , 51 7246445 , 4 171315 , 21 1032360 , 1 210952 , 1 249373 , 2 80456 , 1 13470 , 1 122993 , 1 0 , 1 38362 , 2 19632 , 1 9973 , 1 179161 , 1 23734 , 1 16957'), ('Atol', '1 10898 , 1 52271 , 1 11894 , 2 47490 , 1 13242 , 2 34180'), ('Ayeuen', '1 10520'), ('Bahsa_Seupanyo', '1 18253'), ('Beureukaih:Het_zevende_bataljon_tot_de_aanval_oprukkend.jpg', '1 10320'), ('Beureukaih:Map_mn_khovd_aimag.png', '1 10326')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "AJBwCbKe6XPI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}